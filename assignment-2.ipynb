{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8e9e121-6d5a-42f7-994a-139eb3a1da8c",
   "metadata": {},
   "source": [
    "# Assignment\n",
    "\n",
    "Assigment is in the intersection of Named Entity Recognition and Dependency Parsing.\n",
    "\n",
    "0. Evaluate spaCy NER on CoNLL 2003 data (provided)\n",
    "    - report token-level performance (per class and total)\n",
    "        - accuracy of correctly recognizing all tokens that belong to named entities (i.e. tag-level accuracy) \n",
    "    - report CoNLL chunk-level performance (per class and total);\n",
    "        - precision, recall, f-measure of correctly recognizing all the named entities in a chunk per class and total  \n",
    "\n",
    "1. Grouping of Entities.\n",
    "Write a function to group recognized named entities using `noun_chunks` method of [spaCy](https://spacy.io/usage/linguistic-features#noun-chunks). Analyze the groups in terms of most frequent combinations (i.e. NER types that go together). \n",
    "\n",
    "2. One of the possible post-processing steps is to fix segmentation errors.\n",
    "Write a function that extends the entity span to cover the full noun-compounds. Make use of `compound` dependency relation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d144d32-81d8-4d7e-b431-0132c4eecd0f",
   "metadata": {},
   "source": [
    "# CoNLL Data\n",
    "From https://www.clips.uantwerpen.be/conll2003/ner/\n",
    "\n",
    "The shared task of CoNLL-2003 concerns language-independent named entity recognition. We will concentrate on four types of named entities: persons, locations, organizations and names of miscellaneous entities that do not belong to the previous three groups. \n",
    "\n",
    "The CoNLL-2003 shared task data files contain four columns separated by a single space. Each word has been put on a separate line and there is an empty line after each sentence. The first item on each line is a word, the second a part-of-speech (POS) tag, the third a syntactic chunk tag and the fourth the named entity tag. The chunk tags and the named entity tags have the format I-TYPE which means that the word is inside a phrase of type TYPE. Only if two phrases of the same type immediately follow each other, the first word of the second phrase will have tag B-TYPE to show that it starts a new phrase. A word with tag O is not part of a phrase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5001e6ad-1474-4c13-b410-4d20e006bfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import conll\n",
    "import my_ents\n",
    "\n",
    "# token, POS tag, syntactic chunk tag (IOB), entity tag (IOB)\n",
    "test = conll.read_corpus_conll('dataset/train.txt')\n",
    "test = [my_ents.Sentence(sent) for sent in test if '-DOCSTART-' not in sent[0][0]]\n",
    "# test = test[:100]\n",
    "\n",
    "# count_ = [len(sent) for sent in test]\n",
    "# print(sum(count_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6854e05a-b54a-4eb0-a2c4-6f94f133e693",
   "metadata": {},
   "source": [
    "# 0. Evaluate spaCy NER on CoNLL 2003 data (provided)\n",
    "- report token-level performance (per class and total)\n",
    "    - accuracy of correctly recognizing all tokens that belong to named entities (i.e. tag-level accuracy) \n",
    "- report CoNLL chunk-level performance (per class and total);\n",
    "    - precision, recall, f-measure of correctly recognizing all the named entities in a chunk per class and total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c51bfa-9592-4b9b-b6cf-baf85b3fa21f",
   "metadata": {},
   "source": [
    "Conversion from Ontonotes tags to CoNLL format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6ded94b-69e3-4347-a973-e3b2ab38b10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversion of tags from Ontonotes (spacy) to CoNLL format\n",
    "def from_spacy_to_conll(predictions_spacy):\n",
    "    switcher = {\n",
    "                ' ': '',\n",
    "                '': '',\n",
    "                'ORG': '-ORG',\n",
    "                'PER': '-PER',\n",
    "                'LOC': '-LOC',\n",
    "                'PERSON': '-PER',\n",
    "                'GPE': '-LOC'\n",
    "            }\n",
    "    \n",
    "    # LOC, PER, ORG, MISC\n",
    "    predictions = []\n",
    "    \n",
    "    for sent in predictions_spacy:\n",
    "        new = []\n",
    "        \n",
    "        for ent in sent:\n",
    "            # merge iob and entity type\n",
    "            new.append((ent.text, ent.ent_iob_ + switcher.get(ent.ent_type_, '-MISC')))\n",
    "        \n",
    "        predictions.append(new)\n",
    "        \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14038a9-460d-4f0d-8dc9-609854208049",
   "metadata": {},
   "source": [
    "## Custom tokenizer\n",
    "Define custom tokenizer for spacy, otherwise spaCy will tokenize differently from how the CoNLL dataset has been tokenized. This would produce different tokens in output, rendering impossible to compute the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22c9d87b-b1ac-4c46-8c8e-c544a7ed28bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# def tokenizer_(sent):\n",
    "#     return spacy.tokens.Doc(nlp.vocab, sent.split())\n",
    "\n",
    "nlp.tokenizer = lambda sent: spacy.tokens.Doc(nlp.vocab, sent.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ccaca30-fb7f-4b9c-b6ca-b486263c4714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 23s, sys: 103 ms, total: 1min 23s\n",
      "Wall time: 1min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# spacy predictions\n",
    "predictions = [nlp(str(sent)) for sent in test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "113e4342-2f5a-4da6-94d5-15cfb863619e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('He', 'O'),\n",
       " ('said', 'O'),\n",
       " ('further', 'O'),\n",
       " ('scientific', 'O'),\n",
       " ('study', 'O'),\n",
       " ('was', 'O'),\n",
       " ('required', 'O'),\n",
       " ('and', 'O'),\n",
       " ('if', 'O'),\n",
       " ('it', 'O'),\n",
       " ('was', 'O'),\n",
       " ('found', 'O'),\n",
       " ('that', 'O'),\n",
       " ('action', 'O'),\n",
       " ('was', 'O'),\n",
       " ('needed', 'O'),\n",
       " ('it', 'O'),\n",
       " ('should', 'O'),\n",
       " ('be', 'O'),\n",
       " ('taken', 'O'),\n",
       " ('by', 'O'),\n",
       " ('the', 'B-ORG'),\n",
       " ('European', 'I-ORG'),\n",
       " ('Union', 'I-ORG'),\n",
       " ('.', 'O')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert to NLTK format so that conll.evaluate can be used\n",
    "predictions = from_spacy_to_conll(predictions)\n",
    "\n",
    "predictions[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eadc1cf-89cf-42fa-a760-a02e41d43ac5",
   "metadata": {},
   "source": [
    "## Token-level accuracy, total and per-class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80d7ddb7-7e2e-46d9-8bfb-c9081283bd1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('He', 'O'),\n",
       " ('said', 'O'),\n",
       " ('further', 'O'),\n",
       " ('scientific', 'O'),\n",
       " ('study', 'O'),\n",
       " ('was', 'O'),\n",
       " ('required', 'O'),\n",
       " ('and', 'O'),\n",
       " ('if', 'O'),\n",
       " ('it', 'O'),\n",
       " ('was', 'O'),\n",
       " ('found', 'O'),\n",
       " ('that', 'O'),\n",
       " ('action', 'O'),\n",
       " ('was', 'O'),\n",
       " ('needed', 'O'),\n",
       " ('it', 'O'),\n",
       " ('should', 'O'),\n",
       " ('be', 'O'),\n",
       " ('taken', 'O'),\n",
       " ('by', 'O'),\n",
       " ('the', 'O'),\n",
       " ('European', 'B-ORG'),\n",
       " ('Union', 'I-ORG'),\n",
       " ('.', 'O')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# organize test data in tuples (entity, tag)\n",
    "test_set = [[(ent.text, ent.ent_tag) for ent in sent.ents] for sent in test]\n",
    "\n",
    "test_set[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340637a5-9e5c-4f98-af8d-3bec5f22b6da",
   "metadata": {},
   "source": [
    "### Total accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3492f234-c7d1-4102-830a-684b406772de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8257154222796274"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def total_accuracy(predictions_labels, labels):\n",
    "    if len(predictions_labels) != len(labels):\n",
    "        raise Exception('Prediction labels and test labels have different lenght')\n",
    "    \n",
    "    correct = 0\n",
    "    for i in range(len(predictions_labels)):\n",
    "        if predictions_labels[i] == labels[i]:\n",
    "            correct += 1\n",
    "    \n",
    "    return correct/len(labels)\n",
    "\n",
    "pred_labels = [ent[1] for sent in predictions for ent in sent]\n",
    "test_labels = [ent[1] for sent in test_set for ent in sent]\n",
    "\n",
    "total_accuracy(pred_labels, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cde2a4-b2e7-42ed-a3c7-a7003394fcd8",
   "metadata": {},
   "source": [
    "### Per-class accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a85291b7-d86b-4e74-ba63-778a509e0f00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1-score</th>\n",
       "      <th>support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>B-LOC</th>\n",
       "      <td>0.804400</td>\n",
       "      <td>0.706723</td>\n",
       "      <td>0.752404</td>\n",
       "      <td>7140.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-MISC</th>\n",
       "      <td>0.127121</td>\n",
       "      <td>0.573008</td>\n",
       "      <td>0.208080</td>\n",
       "      <td>3438.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-ORG</th>\n",
       "      <td>0.463032</td>\n",
       "      <td>0.313083</td>\n",
       "      <td>0.373572</td>\n",
       "      <td>6321.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B-PER</th>\n",
       "      <td>0.794085</td>\n",
       "      <td>0.650909</td>\n",
       "      <td>0.715404</td>\n",
       "      <td>6600.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-LOC</th>\n",
       "      <td>0.631902</td>\n",
       "      <td>0.623163</td>\n",
       "      <td>0.627502</td>\n",
       "      <td>1157.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-MISC</th>\n",
       "      <td>0.042681</td>\n",
       "      <td>0.251948</td>\n",
       "      <td>0.072996</td>\n",
       "      <td>1155.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-ORG</th>\n",
       "      <td>0.484502</td>\n",
       "      <td>0.548596</td>\n",
       "      <td>0.514561</td>\n",
       "      <td>3704.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I-PER</th>\n",
       "      <td>0.815482</td>\n",
       "      <td>0.800353</td>\n",
       "      <td>0.807847</td>\n",
       "      <td>4528.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>O</th>\n",
       "      <td>0.952459</td>\n",
       "      <td>0.873781</td>\n",
       "      <td>0.911425</td>\n",
       "      <td>169578.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.825715</td>\n",
       "      <td>0.825715</td>\n",
       "      <td>0.825715</td>\n",
       "      <td>0.825715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>macro avg</th>\n",
       "      <td>0.568407</td>\n",
       "      <td>0.593507</td>\n",
       "      <td>0.553755</td>\n",
       "      <td>203621.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weighted avg</th>\n",
       "      <td>0.894465</td>\n",
       "      <td>0.825715</td>\n",
       "      <td>0.855032</td>\n",
       "      <td>203621.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              precision    recall  f1-score        support\n",
       "B-LOC          0.804400  0.706723  0.752404    7140.000000\n",
       "B-MISC         0.127121  0.573008  0.208080    3438.000000\n",
       "B-ORG          0.463032  0.313083  0.373572    6321.000000\n",
       "B-PER          0.794085  0.650909  0.715404    6600.000000\n",
       "I-LOC          0.631902  0.623163  0.627502    1157.000000\n",
       "I-MISC         0.042681  0.251948  0.072996    1155.000000\n",
       "I-ORG          0.484502  0.548596  0.514561    3704.000000\n",
       "I-PER          0.815482  0.800353  0.807847    4528.000000\n",
       "O              0.952459  0.873781  0.911425  169578.000000\n",
       "accuracy       0.825715  0.825715  0.825715       0.825715\n",
       "macro avg      0.568407  0.593507  0.553755  203621.000000\n",
       "weighted avg   0.894465  0.825715  0.855032  203621.000000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "\n",
    "per_class_metrics = classification_report(test_labels, pred_labels, output_dict=True)\n",
    "pd_tbl_class = pd.DataFrame().from_dict(per_class_metrics).transpose()\n",
    "pd_tbl_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc11b69-f591-473e-88ba-d10aed67ca91",
   "metadata": {},
   "source": [
    "### Chunk-level accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efe5d24e-6d56-44bb-8681-2cdce63ef4f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p</th>\n",
       "      <th>r</th>\n",
       "      <th>f</th>\n",
       "      <th>s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MISC</th>\n",
       "      <td>0.120</td>\n",
       "      <td>0.541</td>\n",
       "      <td>0.196</td>\n",
       "      <td>3438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PER</th>\n",
       "      <td>0.771</td>\n",
       "      <td>0.632</td>\n",
       "      <td>0.695</td>\n",
       "      <td>6600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LOC</th>\n",
       "      <td>0.796</td>\n",
       "      <td>0.699</td>\n",
       "      <td>0.744</td>\n",
       "      <td>7140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORG</th>\n",
       "      <td>0.413</td>\n",
       "      <td>0.279</td>\n",
       "      <td>0.333</td>\n",
       "      <td>6321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total</th>\n",
       "      <td>0.406</td>\n",
       "      <td>0.544</td>\n",
       "      <td>0.465</td>\n",
       "      <td>23499</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           p      r      f      s\n",
       "MISC   0.120  0.541  0.196   3438\n",
       "PER    0.771  0.632  0.695   6600\n",
       "LOC    0.796  0.699  0.744   7140\n",
       "ORG    0.413  0.279  0.333   6321\n",
       "total  0.406  0.544  0.465  23499"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = conll.evaluate(test_set, predictions)\n",
    "\n",
    "pd_tbl = pd.DataFrame().from_dict(results, orient='index')\n",
    "pd_tbl.round(decimals=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
